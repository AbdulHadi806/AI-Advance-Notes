# ML basic notes 

## Terminologies

### Underfitting

<img width="320" alt="image" src="https://github.com/AbdulHadi806/AI-Advance-Notes/assets/113926529/6f34c9ab-0c44-431a-bb09-578cf9753325">

### Overfitting

<img width="320" alt="image" src="https://github.com/AbdulHadi806/AI-Advance-Notes/assets/113926529/c2a7987e-2be5-4752-8d3a-a3f4e92f43af">

### Consequences of Overfitting and Underfitting

Consequences ae

- Inaccuracy: both lead to unreliable predictions
- Underfitting: High Bias, the model consistently gets it wrong in a certain way
- Overfitting: High variance performance wildly depends on the specific data it has seen.

<img width="320" alt="image" src="https://github.com/AbdulHadi806/AI-Advance-Notes/assets/113926529/63fbb6bd-8243-41af-a51c-4781f129b7c8">


### Generalization
Generalization refers to a model's ability to perform well on new, unseen data. When a model generalizes well, it has learned to capture underlying patterns in the data without
simply memorizing specific examples from the training dataset. In other words, the model has learned the underlying relationship between inputs and outputs, allowing it to make accurate predictions
or classifications on new data points. Achieving good generalization involves finding the right balance between model complexity and the amount of training data available. Very
Complex models may memorize noise or irrelevant patterns leading to poor performance on new dataset(overfitting) while simple models may fail to capture important patterns in the data
resulting in underfitting.

# Memorization
This is the term used when a model memorizes the training data rather than underlying patterns, this happens when model is really complex as compared to the training dataset.
This casues overfitting.

